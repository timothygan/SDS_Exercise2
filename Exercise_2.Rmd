---
title: "Exercise 2"
output: github_document
---

```{r, setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(mosaic)
library(FNN)
sclass = read.csv("data/sclass.csv")
news = read.csv("data/online_news.csv")

rmse = function(y, ypred) {
  sqrt(mean(data.matrix((y-ypred)^2)))
}
is.nan.data.frame <- function(x)
  do.call(cbind, lapply(x, is.nan))

pcg = function(table) {
  (table[1] + table[4])/(table[1] + table[2] + table[3] + table[4])
}



k_vs_rmean = NULL
k_vs_rmean_saratoga = NULL
rmse_vals = NULL

n = nrow(SaratogaHouses)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train

set.seed(3)
kframe_s <- data.frame("K" = c(), "RMEAN_AVERAGE" =c())
i <- 3
while(i <= 50){
  avg_cols = do(100)*{
    
    # re-split into train and test cases with the same sample sizes
    train_cases = sample.int(n, n_train, replace=FALSE)
    test_cases = setdiff(1:n, train_cases)
    saratoga_train = SaratogaHouses[train_cases,]
    saratoga_test = SaratogaHouses[test_cases,]
    Xtrain = model.matrix(~ . - (price + sewer + fireplaces + heating) - 1, data=saratoga_train)
    Xtest = model.matrix(~ . - (price + sewer + fireplaces + heating) - 1, data=saratoga_test)
    
    ytrain = saratoga_train$price
    ytest = saratoga_test$price
    
    scale_train = apply(Xtrain, 2, sd)
    Xtilde_train = scale(Xtrain, scale = scale_train)
    Xtilde_test = scale(Xtest, scale = scale_train)
    
    head(Xtrain, 2)
    head(Xtilde_train, 2) %>% round(3)
    knn_model = knn.reg(Xtilde_train, Xtilde_test, ytrain, k=i)
    
    c(rmse(ytest, knn_model$pred))
  }
  d = data.frame("K" = i, "RMEAN_AVERAGE" = mean(avg_cols[["result"]]))
  kframe_s = rbind(kframe_s, d)
  i = i + 1
  
}


k_vs_rmean_saratoga = ggplot(data = kframe_s) + 
  geom_point(mapping = aes(x = K, y = RMEAN_AVERAGE), color='lightgrey') + 
  theme_bw(base_size=18) + geom_path(aes(x = K, y = RMEAN_AVERAGE), color='red')


rmse_vals = do(100)*{
  
  # re-split into train and test cases with the same sample sizes
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  saratoga_train = SaratogaHouses[train_cases,]
  saratoga_test = SaratogaHouses[test_cases,]
  
  # Fit to the training data
  lm1 = lm(price ~ lotSize + bedrooms + bathrooms, data=saratoga_train)
  lm2 = lm(price ~ . - sewer - waterfront - landValue - newConstruction, data=saratoga_train)
  lm3 = lm(price ~ (. - sewer - waterfront - landValue - newConstruction)^2, data=saratoga_train)
  
  lm_dominate = lm(price ~ lotSize + age + livingArea + pctCollege + 
                     bedrooms + fireplaces + bathrooms + rooms + heating + fuel +
                     centralAir + lotSize:heating + livingArea:rooms + newConstruction + livingArea:newConstruction, data=saratoga_train)
  
  

  
  our_model = lm(price ~ . - fireplaces - sewer - heating, data=saratoga_train)
  
  
  #KNN
  Xtrain = model.matrix(~ . - (price + sewer + fireplaces + heating) - 1, data=saratoga_train)
  Xtest = model.matrix(~ . - (price + sewer + fireplaces + heating) - 1, data=saratoga_test)

  ytrain = saratoga_train$price
  ytest = saratoga_test$price

  scale_train = apply(Xtrain, 2, sd)
  Xtilde_train = scale(Xtrain, scale = scale_train)
  Xtilde_test = scale(Xtest, scale = scale_train)

  head(Xtrain, 2)
  head(Xtilde_train, 2) %>% round(3)
  knn_model = knn.reg(Xtilde_train, Xtilde_test, ytrain, k=5)
  
  # Predictions out of sample
  yhat_test1 = predict(lm1, saratoga_test)
  yhat_test2 = predict(lm2, saratoga_test)
  yhat_test3 = predict(lm3, saratoga_test)
  yhat_test4 = predict(lm_dominate, saratoga_test)
  yhat_test5 = predict(our_model, saratoga_test)

  
  c(rmse(saratoga_test$price, yhat_test1),
    rmse(saratoga_test$price, yhat_test2),
    #rmse(saratoga_test$price, yhat_test3),
    rmse(saratoga_test$price, yhat_test4),
    rmse(saratoga_test$price, yhat_test5),
    rmse(ytest, knn_model$pred))
}

set.seed(3)

sum(news$shares > 1400)
sum(news$shares <= 1400)
news$viral[news$shares>1400] <- 1
news$viral[news$shares<=1400] <- 0

n = nrow(news)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train


# re-split into train and test cases with the same sample sizes
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
news_train = news[train_cases,]
news_test = news[test_cases,]
  



Xtrain = model.matrix(~ num_imgs + n_tokens_title + data_channel_is_entertainment +
                        data_channel_is_world + self_reference_avg_sharess + 
                        global_rate_positive_words + global_rate_negative_words +
                        avg_negative_polarity + data_channel_is_socmed, data=news_train)
Xtest = model.matrix(~ num_imgs + n_tokens_title + data_channel_is_entertainment +
                       data_channel_is_world + self_reference_avg_sharess + 
                       global_rate_positive_words + global_rate_negative_words +
                       avg_negative_polarity + data_channel_is_socmed, data=news_test)

ytrain = news_train$shares
ytest = news_test$shares

scale_train = apply(Xtrain, 2, sd)
Xtilde_train = scale(Xtrain, scale = scale_train)
Xtilde_test = scale(Xtest, scale = scale_train)

Xtilde_train[is.nan(Xtilde_train)] <- 0
Xtilde_test[is.nan(Xtilde_test)] <- 0
head(Xtrain, 2)
head(Xtilde_train, 2) %>% round(3)
knn_model = knn.reg(Xtilde_train, Xtilde_test, ytrain, k=5)




news_test_viral = ifelse(news_test$shares > 1400, 1, 0) 

knn_test_viral = ifelse(knn_model$pred > 1400, 1, 0) 

confusion_out_after = table(y = news_test_viral, pred = knn_test_viral)
single_confusion_table_after = confusion_out_after

pec_after = pcg(confusion_out_after)
single_pec_after = pec_after
true_pos_after = confusion_out_after[4]/(confusion_out_after[2] + confusion_out_after[4])
false_pos_after = confusion_out_after[2]/(confusion_out_after[2] + confusion_out_after[4])
single_true_pos_after = true_pos_after
single_false_pos_after = true_pos_after



i <- 3
while(i <= 50){
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  news_train = news[train_cases,]
  news_test = news[test_cases,]
  
  
  
  
  Xtrain = model.matrix(~ num_imgs + n_tokens_title + data_channel_is_entertainment +
                          data_channel_is_world + self_reference_avg_sharess + 
                          global_rate_positive_words + global_rate_negative_words +
                          avg_negative_polarity + data_channel_is_socmed, data=news_train)
  Xtest = model.matrix(~ num_imgs + n_tokens_title + data_channel_is_entertainment +
                         data_channel_is_world + self_reference_avg_sharess + 
                         global_rate_positive_words + global_rate_negative_words +
                         avg_negative_polarity + data_channel_is_socmed, data=news_test)
  
  ytrain = news_train$shares
  ytest = news_test$shares
  
  scale_train = apply(Xtrain, 2, sd)
  Xtilde_train = scale(Xtrain, scale = scale_train)
  Xtilde_test = scale(Xtest, scale = scale_train)
  Xtilde_train[is.nan(Xtilde_train)] <- 0
  Xtilde_test[is.nan(Xtilde_test)] <- 0
  head(Xtrain, 2)
  head(Xtilde_train, 2) %>% round(3)
  knn_model = knn.reg(Xtilde_train, Xtilde_test, ytrain, k=5)
  
  
  
  
  news_test_viral = ifelse(news_test$shares > 1400, 1, 0) 
  
  knn_test_viral = ifelse(knn_model$pred > 1400, 1, 0) 
  
  new_confusion_out = table(y = news_test_viral, o = knn_test_viral)
  
  confusion_out_after[1] = (confusion_out_after[1]+new_confusion_out[1])/2
  confusion_out_after[2] = (confusion_out_after[2]+new_confusion_out[2])/2
  confusion_out_after[3] = (confusion_out_after[3]+new_confusion_out[3])/2
  confusion_out_after[4] = (confusion_out_after[4]+new_confusion_out[4])/2
  true_pos_after = (true_pos_after + new_confusion_out[4]/(new_confusion_out[2] + new_confusion_out[4]))/2
  false_pos_after = (false_pos_after + new_confusion_out[2]/(new_confusion_out[2] + new_confusion_out[4]))/2
  pec_after = (pec_after + pcg(new_confusion_out))/2
  i = i + 1
}




n = nrow(news)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train


# re-split into train and test cases with the same sample sizes
train_cases = sample.int(n, n_train, replace=FALSE)
test_cases = setdiff(1:n, train_cases)
news_train = news[train_cases,]
news_test = news[test_cases,]




Xtrain = model.matrix(~ num_imgs + n_tokens_title + data_channel_is_entertainment +
                        data_channel_is_world + self_reference_avg_sharess + 
                        global_rate_positive_words + global_rate_negative_words +
                        avg_negative_polarity + data_channel_is_socmed, data=news_train)
Xtest = model.matrix(~ num_imgs + n_tokens_title + data_channel_is_entertainment +
                       data_channel_is_world + self_reference_avg_sharess + 
                       global_rate_positive_words + global_rate_negative_words +
                       avg_negative_polarity + data_channel_is_socmed, data=news_test)

ytrain = news_train$viral
ytest = news_test$viral

scale_train = apply(Xtrain, 2, sd)
Xtilde_train = scale(Xtrain, scale = scale_train)
Xtilde_test = scale(Xtest, scale = scale_train)

Xtilde_train[is.nan(Xtilde_train)] <- 0
Xtilde_test[is.nan(Xtilde_test)] <- 0
head(Xtrain, 2)
head(Xtilde_train, 2) %>% round(3)
knn_model = knn.reg(Xtilde_train, Xtilde_test, ytrain, k=5)

knn_test_viral = ifelse(knn_model$pred > .5, 1, 0) 

confusion_out_before = table(y = news_test$viral, pred = knn_test_viral)
single_confusion_out_before = confusion_out_before
pec_before = pcg(confusion_out_before)
single_pec_before = pec_before


true_pos_before = confusion_out_before[4]/(confusion_out_before[2] +confusion_out_before[4])
false_pos_before = confusion_out_before[2]/(confusion_out_before[2] +confusion_out_before[4])
single_true_pos_before = true_pos_before
single_false_pos_before = false_pos_before

i <- 3
while(i <= 50){
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  news_train = news[train_cases,]
  news_test = news[test_cases,]
  
  
  
  
  Xtrain = model.matrix(~ num_imgs + n_tokens_title + data_channel_is_entertainment +
                          data_channel_is_world + self_reference_avg_sharess + 
                          global_rate_positive_words + global_rate_negative_words +
                          avg_negative_polarity + data_channel_is_socmed, data=news_train)
  Xtest = model.matrix(~ num_imgs + n_tokens_title + data_channel_is_entertainment +
                         data_channel_is_world + self_reference_avg_sharess + 
                         global_rate_positive_words + global_rate_negative_words +
                         avg_negative_polarity + data_channel_is_socmed, data=news_test)
  
  ytrain = news_train$viral
  ytest = news_test$viral
  
  scale_train = apply(Xtrain, 2, sd)
  Xtilde_train = scale(Xtrain, scale = scale_train)
  Xtilde_test = scale(Xtest, scale = scale_train)
  Xtilde_train[is.nan(Xtilde_train)] <- 0
  Xtilde_test[is.nan(Xtilde_test)] <- 0
  head(Xtrain, 2)
  head(Xtilde_train, 2) %>% round(3)
  knn_model = knn.reg(Xtilde_train, Xtilde_test, ytrain, k=5)
  
  
  
  
  
  knn_test_viral = ifelse(knn_model$pred > .5, 1, 0) 
  knn_test_null = ifelse(knn_model$pred > 2, 0, 1)
  new_confusion_out = table(y = news_test$viral, pred = knn_test_viral)
  null_model = table(y = news_test$viral, pred = knn_test_null)
  confusion_out_before[1] = (confusion_out_before[1]+new_confusion_out[1])/2
  confusion_out_before[2] = (confusion_out_before[2]+new_confusion_out[2])/2
  confusion_out_before[3] = (confusion_out_before[3]+new_confusion_out[3])/2
  confusion_out_before[4] = (confusion_out_before[4]+new_confusion_out[4])/2
  true_pos_before = (true_pos_before + new_confusion_out[4]/(new_confusion_out[2] + new_confusion_out[4]))/2
  false_pos_before = (false_pos_before + new_confusion_out[2]/(new_confusion_out[2] + new_confusion_out[4]))/2
  pec_before = (pec_before + pcg(new_confusion_out))/2
  i = i + 1
}









```

## KNN Practice

We wil be using the K-nearest neighbors technique to predict the price of Mercedes S Class vehicles based on gas mileage. We will be distinguishing these S Class vehicles by trim. In particular, we will be focusing on just two values of trim: 350 and 65 AMG, and finding optimal values of K for predicting the price of each.

### KNN functions for 350 trim vehicles
```{r sclass_350, echo=FALSE, autodep=TRUE}
# Focus on 2 trim levels: 350 and 65 AMG
sclass350 = subset(sclass, trim == '350')
#dim(sclass350)

N = nrow(sclass350)
N_train = floor(0.8*N)
N_test = N - N_train
train_ind = sample.int(N, N_train, replace=FALSE)

train = sclass350[train_ind,]
test = sclass350[-train_ind,]
test = arrange(test, mileage)

X_train = select(train, mileage)
y_train = select(train, price)
X_test = select(test, mileage)
y_test = select(test, price)


# KNN 250


knn250 = knn.reg(train = X_train, test = X_test, y = y_train, k=250)
ypred_knn250 = knn250$pred

knn3 = knn.reg(train = X_train, test = X_test, y = y_train, k=3)
ypred_knn3 = knn3$pred

knn5 = knn.reg(train = X_train, test = X_test, y = y_train, k=5)
ypred_knn5 = knn5$pred
knn10 = knn.reg(train = X_train, test = X_test, y = y_train, k=10)
ypred_knn10 = knn10$pred
knn20 = knn.reg(train = X_train, test = X_test, y = y_train, k=20)
ypred_knn20 = knn20$pred
knn50 = knn.reg(train = X_train, test = X_test, y = y_train, k=50)
ypred_knn50 = knn50$pred
knn100 = knn.reg(train = X_train, test = X_test, y = y_train, k=100)
ypred_knn100 = knn100$pred

test$ypred_knn250 = ypred_knn250

test$ypred_knn3 = ypred_knn3
test$ypred_knn5 = ypred_knn5
test$ypred_knn10 = ypred_knn10
test$ypred_knn20 = ypred_knn20
test$ypred_knn50 = ypred_knn50
test$ypred_knn100 = ypred_knn100

#p_test_3 = ggplot(data = test) + 
#  geom_point(mapping = aes(x = mileage, y = price), color='lightgrey') + 
#  theme_bw(base_size=18) + geom_path(aes(x = mileage, y = ypred_knn3), color='red')

#p_test_3
#rmse(y_test, ypred_knn3)


#p_test_10 = ggplot(data = test) + 
#  geom_point(mapping = aes(x = mileage, y = price), color='lightgrey') + 
#  theme_bw(base_size=18) + geom_path(aes(x = mileage, y = ypred_knn10), color='red')

#p_test_10
#rmse(y_test, ypred_knn10)

#p_test_50 = ggplot(data = test) + 
#  geom_point(mapping = aes(x = mileage, y = price), color='lightgrey') + 
#  theme_bw(base_size=18) + geom_path(aes(x = mileage, y = ypred_knn50), color='red')

#p_test_50
#rmse(y_test, ypred_knn50)

kframe <- data.frame("K" = c(), "RMEAN" =c())
i <- 3
while (i <= 250) {
  d = data.frame("K" = i, "RMEAN" = rmse(y_test, knn.reg(train = X_train, test = X_test, y = y_train, k=i)$pred))
  
  kframe = rbind(kframe, d)
  i = i + 1

}

k_vs_rmean = ggplot(data = kframe) + 
  geom_point(mapping = aes(x = K, y = RMEAN), color='lightgrey') + 
  theme_bw(base_size=18) + geom_path(aes(x = K, y = RMEAN), color='red')
```

Here we plot the average RMSE for each value of K from 3 to 250, and find that the optimal value of K is `r which(kframe$RMEAN==min(kframe$RMEAN)) + 2` 

```{r sclass_350_2, echo=FALSE, autodep=TRUE}

k_vs_rmean

```

### 65 AMG


```{r sclass_65, echo=FALSE}
# Focus on 2 trim levels: 350 and 65 AMG
sclass65 = subset(sclass, trim == '65 AMG')


N = nrow(sclass65)
N_train = floor(0.8*N)
N_test = N - N_train
train_ind = sample.int(N, N_train, replace=FALSE)

train = sclass65[train_ind,]
test = sclass65[-train_ind,]
test = arrange(test, mileage)
train = arrange(train, mileage)

X_train = select(train, mileage)
y_train = select(train, price)
X_test = select(test, mileage)
y_test = select(test, price)

knn200 = knn.reg(train = X_train, test = X_test, y = y_train, k=200)
ypred_knn200 = knn200$pred

knn3 = knn.reg(train = X_train, test = X_test, y = y_train, k=3)
ypred_knn3 = knn3$pred

knn5 = knn.reg(train = X_train, test = X_test, y = y_train, k=5)
ypred_knn5 = knn5$pred
knn10 = knn.reg(train = X_train, test = X_test, y = y_train, k=10)
ypred_knn10 = knn10$pred
knn20 = knn.reg(train = X_train, test = X_test, y = y_train, k=20)
ypred_knn20 = knn20$pred
knn50 = knn.reg(train = X_train, test = X_test, y = y_train, k=50)
ypred_knn50 = knn50$pred
knn100 = knn.reg(train = X_train, test = X_test, y = y_train, k=100)
ypred_knn100 = knn100$pred

test$ypred_knn200 = ypred_knn200

test$ypred_knn3 = ypred_knn3
test$ypred_knn5 = ypred_knn5
test$ypred_knn10 = ypred_knn10
test$ypred_knn20 = ypred_knn20
test$ypred_knn50 = ypred_knn50
test$ypred_knn100 = ypred_knn100

p_test_3 = ggplot(data = test) + 
  geom_point(mapping = aes(x = mileage, y = price), color='lightgrey') + 
  theme_bw(base_size=18) + geom_path(aes(x = mileage, y = ypred_knn3), color='red')



p_test_10 = ggplot(data = test) + 
  geom_point(mapping = aes(x = mileage, y = price), color='lightgrey') + 
  theme_bw(base_size=18) + geom_path(aes(x = mileage, y = ypred_knn10), color='red')




p_test_50 = ggplot(data = test) + 
  geom_point(mapping = aes(x = mileage, y = price), color='lightgrey') + 
  theme_bw(base_size=18) + geom_path(aes(x = mileage, y = ypred_knn50), color='red')



kframe <- data.frame("K" = c(), "RMEAN" =c())
i <- 3
while (i <= 200) {
  d = data.frame("K" = i, "RMEAN" = rmse(y_test, knn.reg(train = X_train, test = X_test, y = y_train, k=i)$pred))
  kframe = rbind(kframe, d)
  i = i + 1
}
k_vs_rmean = ggplot(data = kframe) + 
  geom_point(mapping = aes(x = K, y = RMEAN), color='lightgrey') + 
  theme_bw(base_size=18) + geom_path(aes(x = K, y = RMEAN), color='red')
```

Here we plot the average RMSE for each value of K from 3 to 200, and find that the optimal value of K is `r which(kframe$RMEAN==min(kframe$RMEAN)) + 2`

```{r sclass_65_2, echo=FALSE, autodep=TRUE}

k_vs_rmean

```

### Conclusion

The optimal value of K is larger for the 350 trim vehicles than the 65 AMG. One explanation for this is that the sample set of 350 trim vehichles is also larger than the set of 65 AMG vehicles. As the value of K gets closer to the size of the entire sample, KNN becomes less useful in estimating the price for a specific mileage value.  

## Saratoga house prices

We will be building and comparing two models to predict the prices of houses in Saratoga, New York. 
The first will be a linear model based on features and feature interactions that we deem important in predicting the prices of houses. The second will be a model that uses the same features as the first but instead uses the K-nearest neighbors technique to make predictions.


### The Data
The data that will be used to build these models is of houses in Saratoga, New York in 2006. The data includes the price of each house and various features that could potentially affect their price such as living area, land value, age, number of rooms, etc. 

### Feature selection
To build these models, we need to determine which features are important in predicting the price of a house, and which can be safely discarded. To start, we decided to not include the sewer type, heating type, and number of fireplaces as these are generally not the primary concerns of people in the market for a new house. Here is a boxplot of how a linear model using these features performed against the 3 sample models discussed in class:

``` {r saratoga1, echo=FALSE, autodep=TRUE, warning=FALSE}
rmse_vals_woutknn = subset(rmse_vals, select = -c(V5) )
boxplot(rmse_vals_woutknn)



```

### Interactions
Next, we need to determine if there are any interactions between the features that we've selected. That is, do any of the features' effect on price change based on the value of another feature. For instance, we predicted that lot size would not have a strong effect on price unless it is significantly larger than the houses living area, otherwise it's effect would be overshadowed by the living area's effect on price. We predicted that the number of bedrooms and bathrooms a house has would also interact with the living area of the house because the two are directly corrleated. However, the model performed worse when considering these interactions, so we decided to discard them in the final model.

### KNN
Now we will attempt to improve on our linear model by using the same features, but instead building the model based on the K-nearest neigbhors technique. This will make a price prediction for a given house based on the K most similar houses in the data set. We will be choosing a value of K by testing which values make the most accurate predictions. Here are values of K plotted against their model performance:

``` {r saratoga2, echo=FALSE, autodep=TRUE, warning=FALSE}

k_vs_rmean_saratoga

```

This shows that the optimal value of k is 5. Here is a boxtplot of the performance of the three sample models, our linear model, and our K-nearest neighbors model using K=5:

``` {r saratoga3, echo=FALSE, autodep=TRUE, warning=FALSE}

boxplot(rmse_vals)
```

As you can see, the K-nearest neighbor model outperformed each of the sample models, but did worse than our linear model. 

### Conclusion
Both the linear and KNN models consistently outperform the sample models, indicating that sewer type, heating type and number of fireplaces do not have a strong effect on the price of houses compared to the rest of the features used.  In addition, two of the sample models discarded waterfront and land value while we included each of these in our models, indicating that one or both are strong indicators of the price of a house. Since it performed the best of the 5 models tested, we recommend using our linear model to predict housing prices going forward.


## Predicting when articles go viral

We will be building two models to predict whether a given article on Mashable.com will go viral or not. An article is classified as viral if it receives greater than 1400 shares. The first model will be trained to predict shares as a target variable, and then classify it as viral if it predicts greater than 1400 shares. The second model will be trained to predict virality as it's target variable, ignoring the number of shares and only focusing on whether or not an article went viral.

### The Data
We will be using data on 39,797 online rticles published by Mashable during 2013 and 2014. This data contains information on each article such as the length of it's title, the length of the article itself, and the positivity/negativity of the words used in the article. The features we will be focusing on to train our two models are listed below:

- num_imgs: Number of images.
- n_tokens_title: Number of words in the title.
- data_channel_is_entertainment: Is the data channel "entertainment"?
- data_channel_is_world: Is the data channel "World"? 
- data_channel_is_socmed: Is the data channel "Social Media"
- self_reference_avg_shares: The average number of shares of other mashable articles linked to from this article
- global_rate_positive_words: Rate of positive words in content
- global_rate_negative_words: Rate of negative words in content
- avg_negative_polarity: average negative polarity of words in content 

Our theory is that articles with shorter titles are less likely to be ignored and thus more likely to be clicked on and shared. We also believe that articles with overtly emotional content (either positive or negative) are more likely to be shared. We chose to include the social media and entertainment channels because viral content tends to be related to either of these categories. The rest of the features such as data_channel_is_world we chose simply because our models tended to perform better with them included.

### Performance
The confusion matrix, overall error rate, true positive rate, and false positive rate for the two models (averaged over many different train/test splits) are shown below:

#### Model 1


``` {r online_news, echo=FALSE, autodep=TRUE, warning=FALSE}
confusion_out_after
```

Overall error rate: `r 1 - pec_after`

True positive rate: `r true_pos_after`

False positve rate: `r false_pos_after`

#### Model 2
``` {r online_news2, echo=FALSE, autodep=TRUE, warning=FALSE}
confusion_out_before
```

Overall error rate: `r 1 - pec_before`

True positive rate: `r true_pos_before`

False positve rate: `r false_pos_before`

#### Null Model

To provide context to each of the models, we provide a "Null Model" that simply predicts that every article will go viral:

``` {r online_news3, echo=FALSE, autodep=TRUE, warning=FALSE}
null_model
```

Overall error rate: `r null_model[1] /(null_model[1] + null_model[2])`

True positive rate: `1.0`

False positive rate: `r null_model[1] / (null_model[1] + null_model[2])`

### Conclusion

The two models have a similar error rate, with the 2nd model performing slightly better. However, the first model has a significantly higher true positive rate. One explanation for this is that since the first model is trained to predict the number of shares for a given article, it is able to account for articles that go very viral (i.e have significanly more than 1400 shares), and give them and their features more weight. However the second model is mostly blind to the number of shares an article received. An article that received exactly 1401 shares and an article that received 10,000 shares are given the same weight in training and prediction. Both models out-perform the null model, suggesting that the features we selected are indicators of whether or not an article will go viral.