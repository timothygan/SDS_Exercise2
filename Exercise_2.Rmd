---
title: "Exercise_2"
output: github_document
---

```{r, setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(mosaic)
library(FNN)
sclass = read.csv("data/sclass.csv")
news = read.csv("data/online_news.csv")

rmse = function(y, ypred) {
  sqrt(mean(data.matrix((y-ypred)^2)))
}
k_vs_rmean = NULL
k_vs_rmean_saratoga = NULL
rmse_vals = NULL

n = nrow(SaratogaHouses)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train

set.seed(3)
kframe_s <- data.frame("K" = c(), "RMEAN_AVERAGE" =c())
i <- 3
while(i <= 50){
  avg_cols = do(100)*{
    
    # re-split into train and test cases with the same sample sizes
    train_cases = sample.int(n, n_train, replace=FALSE)
    test_cases = setdiff(1:n, train_cases)
    saratoga_train = SaratogaHouses[train_cases,]
    saratoga_test = SaratogaHouses[test_cases,]
    Xtrain = model.matrix(~ . - (price + sewer + fireplaces + heating) - 1, data=saratoga_train)
    Xtest = model.matrix(~ . - (price + sewer + fireplaces + heating) - 1, data=saratoga_test)
    
    ytrain = saratoga_train$price
    ytest = saratoga_test$price
    
    scale_train = apply(Xtrain, 2, sd)
    Xtilde_train = scale(Xtrain, scale = scale_train)
    Xtilde_test = scale(Xtest, scale = scale_train)
    
    head(Xtrain, 2)
    head(Xtilde_train, 2) %>% round(3)
    knn_model = knn.reg(Xtilde_train, Xtilde_test, ytrain, k=i)
    
    c(rmse(ytest, knn_model$pred))
  }
  d = data.frame("K" = i, "RMEAN_AVERAGE" = mean(avg_cols[["result"]]))
  print(i)
  print( mean(avg_cols[["result"]]))
  kframe_s = rbind(kframe_s, d)
  i = i + 1
  
}


k_vs_rmean_saratoga = ggplot(data = kframe_s) + 
  geom_point(mapping = aes(x = K, y = RMEAN_AVERAGE), color='lightgrey') + 
  theme_bw(base_size=18) + geom_path(aes(x = K, y = RMEAN_AVERAGE), color='red')


rmse_vals = do(100)*{
  
  # re-split into train and test cases with the same sample sizes
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  saratoga_train = SaratogaHouses[train_cases,]
  saratoga_test = SaratogaHouses[test_cases,]
  
  # Fit to the training data
  lm1 = lm(price ~ lotSize + bedrooms + bathrooms, data=saratoga_train)
  lm2 = lm(price ~ . - sewer - waterfront - landValue - newConstruction, data=saratoga_train)
  lm3 = lm(price ~ (. - sewer - waterfront - landValue - newConstruction)^2, data=saratoga_train)
  
  lm_dominate = lm(price ~ lotSize + age + livingArea + pctCollege + 
                     bedrooms + fireplaces + bathrooms + rooms + heating + fuel +
                     centralAir + lotSize:heating + livingArea:rooms + newConstruction + livingArea:newConstruction, data=saratoga_train)
  
  

  
  our_model = lm(price ~ . - fireplaces - sewer - heating, data=saratoga_train)
  
  
  #KNN
  Xtrain = model.matrix(~ . - (price + sewer + fireplaces + heating) - 1, data=saratoga_train)
  Xtest = model.matrix(~ . - (price + sewer + fireplaces + heating) - 1, data=saratoga_test)

  ytrain = saratoga_train$price
  ytest = saratoga_test$price

  scale_train = apply(Xtrain, 2, sd)
  Xtilde_train = scale(Xtrain, scale = scale_train)
  Xtilde_test = scale(Xtest, scale = scale_train)

  head(Xtrain, 2)
  head(Xtilde_train, 2) %>% round(3)
  knn_model = knn.reg(Xtilde_train, Xtilde_test, ytrain, k=5)
  
  # Predictions out of sample
  yhat_test1 = predict(lm1, saratoga_test)
  yhat_test2 = predict(lm2, saratoga_test)
  yhat_test3 = predict(lm3, saratoga_test)
  yhat_test4 = predict(lm_dominate, saratoga_test)
  yhat_test5 = predict(our_model, saratoga_test)

  
  c(rmse(saratoga_test$price, yhat_test1),
    rmse(saratoga_test$price, yhat_test2),
    #rmse(saratoga_test$price, yhat_test3),
    rmse(saratoga_test$price, yhat_test4),
    rmse(saratoga_test$price, yhat_test5),
    rmse(ytest, knn_model$pred))
}

set.seed(3)


```

## GitHub Documents

This is an R Markdown format used for publishing markdown documents to GitHub. When you click the **Knit** button all R code chunks are run and a markdown file (.md) suitable for publishing to GitHub is generated.

## KNN Practice

We wil be using the K-nearest neighbors technique to predict the price of Mercedes S Class vehicles based on gas mileage. We will be distinguishing these S Class vehicles by trim. In particular, we will be focusing on just two values of trim: 350 and 65 AMG, and finding optimal values of K for predicting the price of each.

### KNN functions for 350 trim vehicles
```{r sclass_350, echo=FALSE, autodep=TRUE}
# Focus on 2 trim levels: 350 and 65 AMG
sclass350 = subset(sclass, trim == '350')
dim(sclass350)

N = nrow(sclass350)
N_train = floor(0.8*N)
N_test = N - N_train
train_ind = sample.int(N, N_train, replace=FALSE)

train = sclass350[train_ind,]
test = sclass350[-train_ind,]
test = arrange(test, mileage)

X_train = select(train, mileage)
y_train = select(train, price)
X_test = select(test, mileage)
y_test = select(test, price)


# KNN 250


knn250 = knn.reg(train = X_train, test = X_test, y = y_train, k=250)
ypred_knn250 = knn250$pred

knn3 = knn.reg(train = X_train, test = X_test, y = y_train, k=3)
ypred_knn3 = knn3$pred

knn5 = knn.reg(train = X_train, test = X_test, y = y_train, k=5)
ypred_knn5 = knn5$pred
knn10 = knn.reg(train = X_train, test = X_test, y = y_train, k=10)
ypred_knn10 = knn10$pred
knn20 = knn.reg(train = X_train, test = X_test, y = y_train, k=20)
ypred_knn20 = knn20$pred
knn50 = knn.reg(train = X_train, test = X_test, y = y_train, k=50)
ypred_knn50 = knn50$pred
knn100 = knn.reg(train = X_train, test = X_test, y = y_train, k=100)
ypred_knn100 = knn100$pred

test$ypred_knn250 = ypred_knn250

test$ypred_knn3 = ypred_knn3
test$ypred_knn5 = ypred_knn5
test$ypred_knn10 = ypred_knn10
test$ypred_knn20 = ypred_knn20
test$ypred_knn50 = ypred_knn50
test$ypred_knn100 = ypred_knn100

p_test_3 = ggplot(data = test) + 
  geom_point(mapping = aes(x = mileage, y = price), color='lightgrey') + 
  theme_bw(base_size=18) + geom_path(aes(x = mileage, y = ypred_knn3), color='red')

p_test_3
rmse(y_test, ypred_knn3)


p_test_10 = ggplot(data = test) + 
  geom_point(mapping = aes(x = mileage, y = price), color='lightgrey') + 
  theme_bw(base_size=18) + geom_path(aes(x = mileage, y = ypred_knn10), color='red')

p_test_10
rmse(y_test, ypred_knn10)

p_test_50 = ggplot(data = test) + 
  geom_point(mapping = aes(x = mileage, y = price), color='lightgrey') + 
  theme_bw(base_size=18) + geom_path(aes(x = mileage, y = ypred_knn50), color='red')

p_test_50
rmse(y_test, ypred_knn50)

kframe <- data.frame("K" = c(), "RMEAN" =c())
i <- 3
while (i <= 250) {
  d = data.frame("K" = i, "RMEAN" = rmse(y_test, knn.reg(train = X_train, test = X_test, y = y_train, k=i)$pred))
  
  kframe = rbind(kframe, d)
  i = i + 1

}

k_vs_rmean = ggplot(data = kframe) + 
  geom_point(mapping = aes(x = K, y = RMEAN), color='lightgrey') + 
  theme_bw(base_size=18) + geom_path(aes(x = K, y = RMEAN), color='red')
```

Here we plot the average RMSE for each value of K from 3 to 250, and find that the optimal value of K is `r which(kframe$RMEAN==min(kframe$RMEAN)) + 2` 

```{r sclass_350_2, echo=FALSE, autodep=TRUE}

k_vs_rmean

```

### 65 AMG


```{r sclass_65, echo=FALSE}
# Focus on 2 trim levels: 350 and 65 AMG
sclass65 = subset(sclass, trim == '65 AMG')
dim(sclass65)

N = nrow(sclass65)
N_train = floor(0.8*N)
N_test = N - N_train
train_ind = sample.int(N, N_train, replace=FALSE)

train = sclass65[train_ind,]
test = sclass65[-train_ind,]
test = arrange(test, mileage)
train = arrange(train, mileage)

X_train = select(train, mileage)
y_train = select(train, price)
X_test = select(test, mileage)
y_test = select(test, price)

knn200 = knn.reg(train = X_train, test = X_test, y = y_train, k=200)
ypred_knn200 = knn200$pred

knn3 = knn.reg(train = X_train, test = X_test, y = y_train, k=3)
ypred_knn3 = knn3$pred

knn5 = knn.reg(train = X_train, test = X_test, y = y_train, k=5)
ypred_knn5 = knn5$pred
knn10 = knn.reg(train = X_train, test = X_test, y = y_train, k=10)
ypred_knn10 = knn10$pred
knn20 = knn.reg(train = X_train, test = X_test, y = y_train, k=20)
ypred_knn20 = knn20$pred
knn50 = knn.reg(train = X_train, test = X_test, y = y_train, k=50)
ypred_knn50 = knn50$pred
knn100 = knn.reg(train = X_train, test = X_test, y = y_train, k=100)
ypred_knn100 = knn100$pred

test$ypred_knn200 = ypred_knn200

test$ypred_knn3 = ypred_knn3
test$ypred_knn5 = ypred_knn5
test$ypred_knn10 = ypred_knn10
test$ypred_knn20 = ypred_knn20
test$ypred_knn50 = ypred_knn50
test$ypred_knn100 = ypred_knn100

p_test_3 = ggplot(data = test) + 
  geom_point(mapping = aes(x = mileage, y = price), color='lightgrey') + 
  theme_bw(base_size=18) + geom_path(aes(x = mileage, y = ypred_knn3), color='red')

p_test_3
rmse(y_test, ypred_knn3)

p_test_10 = ggplot(data = test) + 
  geom_point(mapping = aes(x = mileage, y = price), color='lightgrey') + 
  theme_bw(base_size=18) + geom_path(aes(x = mileage, y = ypred_knn10), color='red')

p_test_10
rmse(y_test, ypred_knn10)


p_test_50 = ggplot(data = test) + 
  geom_point(mapping = aes(x = mileage, y = price), color='lightgrey') + 
  theme_bw(base_size=18) + geom_path(aes(x = mileage, y = ypred_knn50), color='red')

p_test_50
rmse(y_test, ypred_knn50)

kframe <- data.frame("K" = c(), "RMEAN" =c())
i <- 3
while (i <= 200) {
  d = data.frame("K" = i, "RMEAN" = rmse(y_test, knn.reg(train = X_train, test = X_test, y = y_train, k=i)$pred))
  kframe = rbind(kframe, d)
  i = i + 1
}
k_vs_rmean = ggplot(data = kframe) + 
  geom_point(mapping = aes(x = K, y = RMEAN), color='lightgrey') + 
  theme_bw(base_size=18) + geom_path(aes(x = K, y = RMEAN), color='red')
```

Here we plot the average RMSE for each value of K from 3 to 200, and find that the optimal value of K is `r which(kframe$RMEAN==min(kframe$RMEAN)) + 2`

```{r sclass_65_2, echo=FALSE, autodep=TRUE}

k_vs_rmean

```

### Conclusion

The optimal value of K is larger for the 350 trim vehicles than the 65 AMG. One explanation for this is that the sample set of 350 trim vehichles is also larger than the set of 65 AMG vehicles. As the value of K gets closer to the size of the entire sample, KNN becomes less useful in estimating the price for a specific mileage value.  

## Saratoga house prices

We will be building and comparing two models to predict the prices of houses in Saratoga, New York. 
The first will be a linear model based on features and feature interactions that we deem important in predicting the prices of houses. The second will be a model that uses the same features as the first but instead uses the K-nearest neighbors technique to make predictions.


### The Data
The data that will be used to build these models is of houses in Saratoga, New York in 2006. The data includes the price of each house and various features that could potentially affect their price such as living area, land value, age, number of rooms, etc. 

### Feature selection
To build these models, we need to determine which features are important in predicting the price of a house, and which can be safely discarded. To start, we decided to not include the sewer type, heating type, and number of fireplaces as these are generally not the primary concerns of people in the market for a new house. Here is a boxplot of how a linear model using these features performed against the 3 sample models discussed in class:

``` {r saratoga1, echo=FALSE, autodep=TRUE, warning=FALSE}
rmse_vals_woutknn = subset(rmse_vals, select = -c(V5) )
boxplot(rmse_vals_woutknn)



```

### Interactions
Next, we need to determine if there are any interactions between the features that we've selected. That is, do any of the features' effect on price change based on the value of another feature. For instance, we predicted that lot size would not have a strong effect on price unless it is significantly larger than the houses living area, otherwise it's effect would be overshadowed by the living area's effect on price. We predicted that the number of bedrooms and bathrooms a house has would also interact with the living area of the house because the two are directly corrleated. However, the model performed worse when considering these interactions, so we decided to discard them in the final model.

### KNN
Now we will attempt to improve on our linear model by using the same features, but instead building the model based on the K-nearest neigbhors technique. This will make a price prediction for a given house based on the K most similar houses in the data set. We will be choosing a value of K by testing which values make the most accurate predictions. Here are values of K plotted against their model performance:

``` {r saratoga2, echo=FALSE, autodep=TRUE, warning=FALSE}

k_vs_rmean_saratoga

```

This shows that the optimal value of k is 5. Here is a boxtplot of the performance of the three sample models, our linear model, and our K-nearest neighbors model using K=5:

``` {r saratoga3, echo=FALSE, autodep=TRUE, warning=FALSE}

boxplot(rmse_vals)
```

As you can see, the K-nearest neighbor model outperformed each of the sample models, but did worse than our linear model. 

### Conclusion
Both the linearn and KNN models consistently outperform the sample models, indicating that sewer type, heating type and number of fireplaces do not have a strong effect on the price of houses compared to the rest of the features used.  In addition, two of the sample models discarded waterfront and land value while we included each of these in our models, indicating that one or both are strong indicators of the price of a house. Since it performed the best of the 5 models tested, we recommend using our linear model to predict housing prices going forward.

``` {r online_news, echo=FALSE, autodep=TRUE, warning=FALSE}
n = nrow(news)
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
n_train = round(0.8*n)  # round to nearest integer
n_test = n - n_train
rmse_vals = do(100)*{
  
  # re-split into train and test cases with the same sample sizes
  train_cases = sample.int(n, n_train, replace=FALSE)
  test_cases = setdiff(1:n, train_cases)
  news_train = news[train_cases,]
  news_test = news[test_cases,]
  
  # Fit to the training data

  our_model = lm(shares ~ n_tokens_title + num_imgs + num_keywords + 
                   data_channel_is_entertainment + data_channel_is_tech +
                   weekday_is_friday + weekday_is_saturday + 
                   weekday_is_sunday + title_subjectivity + title_sentiment_polarity + avg_negative_polarity, data=news_train)

  
  # Predictions out of sample

  yhat_test = predict(our_model, news_test)

  
  c(rmse(news_test$shares, yhat_test))
}

```
